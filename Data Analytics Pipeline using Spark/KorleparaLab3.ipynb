{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ratnamala Korlepara\n",
    "#Hima Sujani Adike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pyspark\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import sys\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from nytimesarticle import articleAPI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from nytimesarticle import articleAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Articles Collection</h1><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics=[\"trump\",\"nfl\",\"walmart\",\"google\"]\n",
    "subjects=[\"politics\",\"sports\",\"business\",\"other\"]\n",
    "\n",
    "n = articleAPI('44cccf540f9c4f51a97065dcfc6c8bd1')\n",
    "index=0\n",
    "\n",
    "def getArticlesURLs(topic):\n",
    "    articles = n.search(q = topic[index], \n",
    "         fq = 'The New York Times')\n",
    "    articles = articles['response']['docs']\n",
    "    urls=[]\n",
    "    for article in articles:\n",
    "        urls.append(article['web_url'])\n",
    "    return urls\n",
    "\n",
    "for topic in topics:\n",
    "    urls=getArticlesURLs(topic)\n",
    "    i=0\n",
    "    for url in urls:\n",
    "        file=open(subjects[index]+str(i)+'.txt','w+')\n",
    "        i=i+1\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        for line in soup.find_all('p'):\n",
    "            line=str(line)\n",
    "            file.write(line)\n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Logistic Regression</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>1. Creating new Spark and SQL context</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>2. Reading stopwords from a file</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "stop_words=[]\n",
    "file=open('stopwords.txt','r')\n",
    "for line in file:\n",
    "    new_line=line.replace(\"\\n\",\"\")\n",
    "    line1=new_line\n",
    "    stop_words.append(line1)\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>3. Checking if a word is Stop word</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isNotStopWord(word,stopWords):\n",
    "    for stopword in stopWords:\n",
    "        if stopword==word:\n",
    "            return 0\n",
    "    return 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Retrieving topic by filename</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics=[\"politics\",\"sports\",\"business\",\"other\"]\n",
    "def getArticleTopic(file):\n",
    "    for topic in topics:\n",
    "        if topic in file:\n",
    "            return topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>5. Reading all articles</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataList=[]\n",
    "testList=[]\n",
    "i=0\n",
    "for file in glob.glob(\"/articles/*\"):        \n",
    "    i=i+1\n",
    "    with open(file, 'rb') as f:\n",
    "        data=f.read()\n",
    "        words=[]\n",
    "        word_tokens = data.split()\n",
    "        for word in word_tokens:\n",
    "            word=word.decode(\"utf-8\", errors='ignore') \n",
    "            topic=getArticleTopic(file)\n",
    "            if isNotStopWord(word,stop_words):\n",
    "                words.append(word)\n",
    "                words.append(\" \")\n",
    "        dataList.append((i,''.join(words),topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6. Creating a Dataframe of input data and topic</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+\n",
      "|index|                data|   topic|\n",
      "+-----+--------------------+--------+\n",
      "|    1|\"css-8ncjap etcg8...|  sports|\n",
      "|    2|alt.activism.d:77...|politics|\n",
      "|    3|talk.politics.mis...|politics|\n",
      "|    4|\"css-1cbhw1y e1x1...|business|\n",
      "|    5|\"css-1cbhw1y e1x1...|   other|\n",
      "+-----+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = sqlContext.createDataFrame(dataList, [\"index\", \"data\",\"topic\"])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>7. Splitting data and converting into matrix of token counts</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"data\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>8. Creating pipeline of stages in the order of execution</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>9. Fitting Training Data to pipeline</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|index|                data|   topic|               words|            filtered|            features|label|\n",
      "+-----+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|    1|\"css-8ncjap etcg8...|  sports|[css, 8ncjap, etc...|[css, 8ncjap, etc...|(8789,[0,1,2,3,5,...|  3.0|\n",
      "|    2|alt.activism.d:77...|politics|[alt, activism, d...|[alt, activism, d...|(8789,[4,10,11,12...|  0.0|\n",
      "|    3|talk.politics.mis...|politics|[talk, politics, ...|[talk, politics, ...|(8789,[4,12,19,21...|  0.0|\n",
      "|    4|\"css-1cbhw1y e1x1...|business|[css, 1cbhw1y, e1...|[css, 1cbhw1y, e1...|(8789,[0,1,2,11,1...|  1.0|\n",
      "|    5|\"css-1cbhw1y e1x1...|   other|[css, 1cbhw1y, e1...|[css, 1cbhw1y, e1...|(8789,[0,1,2,3,4,...|  2.0|\n",
      "+-----+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>10. Splitting into training and test data</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>11. Training model using Logistic Regression</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>12. Testing model using test data</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                          data|   topic|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|cantaloupe.srv.cs.cmu.edu!c...|politics|[0.9999982738791914,7.07392...|  0.0|       0.0|\n",
      "|\"css-1cbhw1y e1x1pwtg1\"By!-...|politics|[0.9929010222281837,0.00205...|  0.0|       0.0|\n",
      "|talk.politics.misc cantalou...|politics|[0.9897115499808609,0.00266...|  0.0|       0.0|\n",
      "|talk.abortion:118687 talk.p...|politics|[0.9802957421944439,0.00589...|  0.0|       0.0|\n",
      "|misc.headlines:41588 talk.p...|politics|[0.9771546358583013,0.00688...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"data\",\"topic\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 5, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>13. Accuracy calculation for model using Logistic Regression</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Logistic Regression: 98.20367955961177\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Accuracy using Logistic Regression:',evaluator.evaluate(predictions)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Decision Tree Classification </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Split the input data<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+--------------------+\n",
      "|index|                data|   topic|               words|\n",
      "+-----+--------------------+--------+--------------------+\n",
      "|    1|\"css-8ncjap etcg8...|  sports|[\"css-8ncjap, etc...|\n",
      "|    2|alt.activism.d:77...|politics|[alt.activism.d:7...|\n",
      "|    3|talk.politics.mis...|politics|[talk.politics.mi...|\n",
      "|    4|\"css-1cbhw1y e1x1...|business|[\"css-1cbhw1y, e1...|\n",
      "|    5|\"css-1cbhw1y e1x1...|   other|[\"css-1cbhw1y, e1...|\n",
      "+-----+--------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"data\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(data)\n",
    "wordsData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>2. Removing Stop words and converting data into matrix of token counts</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"data\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features1\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>3. Creating pipeline of stages in the order of execution</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>4. Fitting Training Data to pipeline</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|index|                data|   topic|               words|            filtered|         rawFeatures|            features|label|\n",
      "+-----+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|    1|\"css-8ncjap etcg8...|  sports|[css, 8ncjap, etc...|[css, 8ncjap, etc...|(10000,[1,5,20,25...|(10000,[1,5,20,25...|  3.0|\n",
      "|    2|alt.activism.d:77...|politics|[alt, activism, d...|[alt, activism, d...|(10000,[8,14,63,8...|(10000,[8,14,63,8...|  0.0|\n",
      "|    3|talk.politics.mis...|politics|[talk, politics, ...|[talk, politics, ...|(10000,[67,274,31...|(10000,[67,274,31...|  0.0|\n",
      "|    4|\"css-1cbhw1y e1x1...|business|[css, 1cbhw1y, e1...|[css, 1cbhw1y, e1...|(10000,[45,63,70,...|(10000,[45,63,70,...|  1.0|\n",
      "|    5|\"css-1cbhw1y e1x1...|   other|[css, 1cbhw1y, e1...|[css, 1cbhw1y, e1...|(10000,[7,24,36,7...|(10000,[7,24,36,7...|  2.0|\n",
      "+-----+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>5. Splitting into training and test data</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dtModel = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>6. Testing model using test data</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+-----------------+-----+----------+\n",
      "|                          data|   topic|      probability|label|prediction|\n",
      "+------------------------------+--------+-----------------+-----+----------+\n",
      "|alt.fan.rush-limbaugh:23020...|politics|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|alt.politics.clinton:29393 ...|politics|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|talk.politics.misc:176878 a...|politics|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|alt.conspiracy:21826 alt.ac...|politics|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|cantaloupe.srv.cs.cmu.edu!c...|politics|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "+------------------------------+--------+-----------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions1 = dtModel.transform(testData)\n",
    "predictions1.filter(predictions1['prediction'] == 0) \\\n",
    "    .select(\"data\",\"topic\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 5, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>7. Accuracy calculation for model using Decision Tree</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Decision tree: 94.63402094981042\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Accuracy using Decision tree:',evaluator.evaluate(predictions1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
